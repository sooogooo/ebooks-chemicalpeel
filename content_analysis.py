#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import glob
from pathlib import Path

def count_chinese_chars(text):
    """ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°"""
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
    return len(chinese_pattern.findall(text))

def analyze_content():
    """åˆ†æå†…å®¹å®Œæ•´æ€§å’Œç»Ÿè®¡ä¿¡æ¯"""
    
    docs_path = Path("docs")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_files': 0,
        'total_chinese_chars': 0,
        'total_words': 0,
        'chapters': [],
        'missing_files': [],
        'broken_links': []
    }
    
    # é¢„æœŸçš„ç« èŠ‚æ–‡ä»¶
    expected_chapters = [
        '01_know_your_skin.md',
        '02_acid_science.md', 
        '03_acid_encyclopedia.md',
        '04_preparation.md',
        '05_beginner_guide.md',
        '06_targeted_solutions.md',
        '07_product_selection.md',
        '08_care_during_process.md',
        '09_medical_grade_peels.md',
        '10_safety_risk_control.md',
        '11_evaluation_adjustment.md',
        '12_lifestyle_management.md',
        '13_makeup_balance.md',
        '14_community_sharing.md'
    ]
    
    print("=== ã€Šåˆ·é…¸åŒ»ç¾ï¼šåˆ·å‡ºå¥½æ°”è‰²ã€‹å†…å®¹åˆ†ææŠ¥å‘Š ===\n")
    
    # æ£€æŸ¥ç« èŠ‚å®Œæ•´æ€§
    print("ğŸ“š ç« èŠ‚å®Œæ•´æ€§æ£€æŸ¥:")
    chapters_path = docs_path / "chapters"
    
    for chapter in expected_chapters:
        chapter_file = chapters_path / chapter
        if chapter_file.exists():
            with open(chapter_file, 'r', encoding='utf-8') as f:
                content = f.read()
                chinese_chars = count_chinese_chars(content)
                word_count = len(content.split())
                
                stats['chapters'].append({
                    'file': chapter,
                    'chinese_chars': chinese_chars,
                    'word_count': word_count,
                    'size': len(content)
                })
                
                stats['total_chinese_chars'] += chinese_chars
                stats['total_words'] += word_count
                stats['total_files'] += 1
                
                print(f"  âœ… {chapter} - {chinese_chars:,} ä¸­æ–‡å­—ç¬¦")
        else:
            stats['missing_files'].append(chapter)
            print(f"  âŒ {chapter} - æ–‡ä»¶ç¼ºå¤±")
    
    # æ£€æŸ¥å…¶ä»–é‡è¦æ–‡ä»¶
    print("\nğŸ“„ å…¶ä»–æ–‡ä»¶æ£€æŸ¥:")
    other_files = [
        'index.md',
        'preface.md', 
        'introduction.md',
        'epilogue.md'
    ]
    
    for file in other_files:
        file_path = docs_path / file
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                chinese_chars = count_chinese_chars(content)
                stats['total_chinese_chars'] += chinese_chars
                stats['total_files'] += 1
                print(f"  âœ… {file} - {chinese_chars:,} ä¸­æ–‡å­—ç¬¦")
        else:
            print(f"  âŒ {file} - æ–‡ä»¶ç¼ºå¤±")
    
    # æ£€æŸ¥é™„å½•æ–‡ä»¶
    print("\nğŸ“‹ é™„å½•æ–‡ä»¶æ£€æŸ¥:")
    appendix_path = docs_path / "appendix"
    if appendix_path.exists():
        for file in appendix_path.glob("*.md"):
            with open(file, 'r', encoding='utf-8') as f:
                content = f.read()
                chinese_chars = count_chinese_chars(content)
                stats['total_chinese_chars'] += chinese_chars
                stats['total_files'] += 1
                print(f"  âœ… {file.name} - {chinese_chars:,} ä¸­æ–‡å­—ç¬¦")
    
    # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶
    print("\nğŸ–¼ï¸  å›¾ç‰‡èµ„æºæ£€æŸ¥:")
    images_path = docs_path / "images"
    if images_path.exists():
        svg_files = list(images_path.glob("*.svg"))
        png_files = list(images_path.glob("*.png"))
        jpg_files = list(images_path.glob("*.jpg"))
        
        print(f"  ğŸ“Š SVGå›¾è¡¨: {len(svg_files)} ä¸ª")
        print(f"  ğŸ–¼ï¸  PNGå›¾ç‰‡: {len(png_files)} ä¸ª") 
        print(f"  ğŸ“· JPGå›¾ç‰‡: {len(jpg_files)} ä¸ª")
    
    # ç»Ÿè®¡æ€»ç»“
    print(f"\nğŸ“Š ç»Ÿè®¡æ€»ç»“:")
    print(f"  ğŸ“ æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"  ğŸ”¤ ä¸­æ–‡å­—ç¬¦æ€»æ•°: {stats['total_chinese_chars']:,}")
    print(f"  ğŸ“– é¢„ä¼°é˜…è¯»æ—¶é—´: {stats['total_chinese_chars'] // 400:.0f} åˆ†é’Ÿ")
    print(f"  ğŸ“„ é¢„ä¼°é¡µæ•°: {stats['total_chinese_chars'] // 500:.0f} é¡µ")
    
    # ç« èŠ‚å­—æ•°åˆ†å¸ƒ
    print(f"\nğŸ“ˆ ç« èŠ‚å­—æ•°åˆ†å¸ƒ:")
    for chapter in stats['chapters']:
        print(f"  {chapter['file']}: {chapter['chinese_chars']:,} å­—ç¬¦")
    
    # å†…å®¹è´¨é‡æ£€æŸ¥
    print(f"\nğŸ” å†…å®¹è´¨é‡æ£€æŸ¥:")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ–‡ä»¶
    empty_files = [ch for ch in stats['chapters'] if ch['chinese_chars'] < 100]
    if empty_files:
        print(f"  âš ï¸  ç–‘ä¼¼ç©ºæ–‡ä»¶: {len(empty_files)} ä¸ª")
        for f in empty_files:
            print(f"    - {f['file']}")
    else:
        print(f"  âœ… æ— ç©ºæ–‡ä»¶")
    
    # æ£€æŸ¥ç« èŠ‚é•¿åº¦å‡è¡¡æ€§
    avg_length = stats['total_chinese_chars'] / len(stats['chapters']) if stats['chapters'] else 0
    short_chapters = [ch for ch in stats['chapters'] if ch['chinese_chars'] < avg_length * 0.5]
    long_chapters = [ch for ch in stats['chapters'] if ch['chinese_chars'] > avg_length * 2]
    
    if short_chapters:
        print(f"  ğŸ“ åçŸ­ç« èŠ‚ (<{avg_length*0.5:.0f}å­—): {len(short_chapters)} ä¸ª")
    if long_chapters:
        print(f"  ğŸ“ åé•¿ç« èŠ‚ (>{avg_length*2:.0f}å­—): {len(long_chapters)} ä¸ª")
    
    if not short_chapters and not long_chapters:
        print(f"  âœ… ç« èŠ‚é•¿åº¦åˆ†å¸ƒå‡è¡¡")
    
    return stats

if __name__ == "__main__":
    analyze_content()
